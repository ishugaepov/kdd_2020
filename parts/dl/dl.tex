\begin{bibunit}[plainnat]

\input{chapters/retrospec.tex}

\input{chapters/correlation.tex}

% \chapter{Taming Pretrained Transformers for eXtreme Multi-label Text Classification}

\chapter{Другие работы}

\section*{Facebook: Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems}

\textbf{Refernce:}~\url{https://arxiv.org/pdf/1909.02107.pdf}

\textbf{Keywords:} recommendation systems, embeddings, model compression \\

Современные deep learning модели, которые используются в рекомендательных системах, при обучении используют большое количество категориальных признаков большой размерности. 

Как правило, для каждого категориального признака обучается матрица эмбеддингов, которая хранит вектора для каждого возможного значения признака. \\

Если размерность признака большая, то размер матрицы эмбеддингов может легко доходить до нескольких Гб. Это создает определенные проблемы как при обучении так и при использовании сети. \\

Один из самых простых способов уменьшить размер модели --- hashing trick, значения категориального признака хэшируются в меньшее пространство. Hashing trick очень просто в использовании, но коллизии при хэшировании приводят к потере качества. \\

В статье представлен способ уменьшения размера модели за счет хранения не одной большой матрицы эмбеддингов для каждого признака, а нескольких небольших матриц.

В рамках экспериментов, авторы показывают, что описанный способ позволяет уменьшить размер модели до 4х раз при этом почти не потеряв в качестве.

\section*{Improving Deep Learning For Airbnb Search}

\textbf{Reference:}~\cite{haldar2020improving} \\

Продолжение статьи Applying Deep Learning To Airbnb Search~\url{https://arxiv.org/pdf/1810.09591.pdf} \\

Довольно интересная статья про переход от GBDT к DL моделям в поиске Airbnb. \\

В отличии от большинства статей, в данной работе авторы большое внимание уделяют тому какие шишки они набили в процессе перехода к DL и про опыт в целом, а не просто тому какую модель они задеплоили. Не мало места они уделяют обзору того, что хорошо работает в статьях, но у них не заработало. 

Особенно мотивирующим получился раздел RETROSPECTIVE. 


\addcontentsline{toc}{chapter}{Литература}
\putbib[refs_dl]
\end{bibunit}
