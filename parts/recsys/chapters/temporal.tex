\chapter{Temporal-Contextual Recommendation in Real-Time}

Amazon \\

\textbf{Best ADS paper}

\textbf{Reference:}~\url{https://dl.acm.org/doi/pdf/10.1145/3394486.3403278}

\textbf{Keywords:} recommender systems, recurrent neural networks, hybrid model

\section{Какую задачу решают авторы?}

Разработчики современных рекомендательных систем сталкиваются со следующими челенджами: Система должна

\begin{itemize}
    \item Оперативно реагировать на изменение интересов пользователя
    \item Обучаться на историях большого числа пользователей, состоящих из сотен событий, за разумное время
    \item Быть эффективной для новых пользователей и объектов (cold-start problem)
    \item Хорошо масштабироваться на случай ранжирования большого числа объектов
\end{itemize}

Многие популярные решения не удовлетворяют всем указанным требованиям. 

Например, решения, в основе которых лежит факторизация матрицы \texttt{Users-Items}, не позволяют оперативно реагировать на изменение интересов пользователя, и не могут строить рекомендации для новых пользователей/объектов. \\

В статье авторы предлагают решение, которое удовлетворяет всем перечисленным требованиям.

В основе решения --- \texttt{RNN-like} архитектура, которая наряду с идентификаторами пользователя и объекта использует доступную мета-информацию (признаки пользователя и объекта).

Для того чтобы модель можно было за разумное время обучить на большом каталоге объектов, авторы предлагают использовать \texttt{Negative Sampling}~\cite{mikolov2013distributed} (NS) вместо классической многоклассовой классификации.

\section{Как решают?}

Не смотря на то, что в статье описано много интересного, я хотел бы уделить внимание только одному моменту в работе --- использование NS для ускорения обучения.

\subsection{Negative Sampling}

Сначала ответим на вопрос почему обучение моделей рекомендаций при большом размере каталога товаров может быть медленным. \\

Рассмотрим следующую постановку задачи рекомендаций: пусть у нас есть история пользователя $ \{i_1, \ldots, i_k \} $, например, какие товары он уже купил, наша задача научиться предсказывать следующий товар $i_{k+1}$, который заинтерисует пользователя. \\

Довольно часто задачу в такой постановке решают сводя ее к задаче \textit{многоклассовой классификации}, где классам соответствуют товары из каталога. 

Детальнее, пусть каталог состоит из $m$ товаров,
\begin{enumerate}
    \item Используя модель, мы получаем скрытое представление $\vech_k \coloneqq \vech(i_1, \ldots, i_k) \in \bR^{d} $, описывающее историю пользователя
    \item Полученное представление $\vech_k$ домножаем на матрицу $W \in \bR^{m \times d}$, чтобы получить скор для каждого товара в каталоге
    \item Итоговые вероятности для товаров 
        \begin{equation*}
            \hat{y} \coloneqq \softmax (W \cdot \vech_k)
        \end{equation*}
\end{enumerate}

Модель $\vech$ и матрица $W$ обучаются оптимизируя \texttt{Cross Entropy Loss}. \\

Домножение вектора $\vech_k$ на матрицу $W$ с последующим softmax'ом занимает много времени, кроме того, матрица $W$ занимает очень много места. \\

При использовании NS~\cite{mikolov2013distributed} нам не нужно вычислять скоры для всех товаров, а только для релевантного товара и для небольшого случайного множества негативных примеров, так как решаем задачу \textit{бинарной классификации} --- учимся отличать релевантные товары от нерелавнтных.

Это позволяет не хранить матрицу $W$ и существенно ускоряет процесс обучения.

\subsection{Experiments}

В рамках экспериментов авторы исследуют то как зависит скорость обучения (\#items/sec) при использовании NS от размера каталога. \\

Авторы замерели скорость обучения и метрики на датасетах MovieLens (1m, 10m, 20m) и на датасете Taobao, при использовании Dense output слоя и при использовании NS.

На датасетах MovieLens модель обучали в течении четырех эпох. На датасете Taobao время на обучение было ограничено шестью часами. \\

Исходя из результатов (см. таблицу~\ref{table:temporal}), можно сделать вывод, что использование NS существенно ускоряет обучение в случае большого каталога объектов (Taobao).

Кроме того, при ограничениях на время обучения, использование NS позволяет добиться лучшего качества за счет того, что модель успевает обработать большее число объектов. 

\begin{figure}
    \centering
    \begin{tabular}{cccccc}
        \hline & Output & ml-1m & ml-10m & ml-20m & Taobao \\
        \hline Output size & IS & 62 & 255 & 362 & 1087 \\
        $(m)$ & Dense & 1683 & 65134 & 131263 & 1183451 \\
        \hline Throughput & NS & $\underline{23 \mathrm{k}}$ & $20 \mathrm{k}$ & $\mathbf{2 0 k}$ & $\mathbf{7 . 8 k}$ \\
        (\#items/sec) & Dense & $23 \mathrm{k}$ & $23 \mathrm{k}$ & $17 \mathrm{k}$ & 631 \\
        \hline PPL & NS & $\underline{377}$ & $\underline{405}$ & $\underline{455}$ & $\mathbf{1 7 . 6 k}$ \\
        & Dense & 409 & 439 & 494 & $119 \mathrm{k}$ \\
        \hline NDCG & NS & 0.128 & 0.123 & 0.12 & $\mathbf{0 . 1 5}$ \\
        & Dense & 0.123 & 0.119 & 0.115 & 0.08 \\
        \hline
    \end{tabular}
    \caption{\footnotesize{Model throughput and accuracy. Bold-fonts/underlines show significant/insignificant differences, respectively. NS significantly improved model training for up to 1M unique items (Taobao dataset). Additionally, the dense model for the Taobao dataset requires >16GB memory for BPTT, which is infeasible on many GPUs.}}
    \label{table:temporal}
\end{figure}

\newpage

Наряду с привычными метриками для данной задачи, в статье используют еще и \textbf{Perplexity}.

\textbf{Remark:} A recommendation system with a PPL of $p$ is equivalent to one that recommends a uniform random selection of $p$ items, one of which is the true next item.

\section{Выводы}

\dbox{\textbf{Key Takeaways}:
\begin{enumerate}
    \item Если каталог объектов очень большой ($\geq 10^5$), то для обучения модели ранжировани лучше использовать \texttt{Negative Sampling}.
    Это позволяет не только ускорить обучение, но и существенно уменьшить размер модели.
\end{enumerate}}